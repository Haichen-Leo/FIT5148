{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TaskC.2 Stream Processing \n",
    "Processing stream data from Kafka producers by Apache Spark Streaming.\n",
    "Part of codes are taken from Week12_Spark_Stream_Processor.ipynb.\n",
    "\n",
    "This application use Hash Join to implement streaming join function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from datetime import datetime\n",
    "\n",
    "# Drop test value\n",
    "client = MongoClient()\n",
    "db = client.fit5148_assignment\n",
    "collStream = db.stream\n",
    "collStream.drop()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geohash for 42.6, -5.6: ezs42\n"
     ]
    }
   ],
   "source": [
    "# install python-geohash to implement geo-hashing algorithm.\n",
    "# !pip3 install python-geohash\n",
    "\n",
    "import Geohash\n",
    "print ('Geohash for 42.6, -5.6:', Geohash.encode(42.6, -5.6, precision=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def climate_handler(geohash_dictionary, climate_temp, data):\n",
    "    climate = {}\n",
    "    climate['climate_station'] = str(data.get('station'))\n",
    "    climate['date'] = datetime.strptime(data.get('create_date'),'%Y/%m/%d %H:%M:%S')\n",
    "    climate['air_temperature_celcius'] = float(data.get('air_temperature_celcius'))\n",
    "    climate['relative_humidity'] = float(data.get('relative_humidity'))\n",
    "    climate['windspeed'] = float(data.get('windspeed_knots'))\n",
    "    climate['max_wind_speed'] = float(data.get('max_wind_speed'))\n",
    "    climate['precipitation'] = float(data.get('precipitation ')[:-1]) # mind the space ' '\n",
    "    climate['precipitation_flag'] = data.get('precipitation ')[-1]\n",
    "    climate['longitude'] = float(data.get('longitude'))\n",
    "    climate['latitude'] = float(data.get('latitude'))\n",
    "    climate['fire'] = []\n",
    "    \n",
    "    # Map hotspot records based on geohash value\n",
    "    geohash = Geohash.encode(climate['latitude'], climate['longitude'], precision=5)\n",
    "    \n",
    "    # For climate records of the same location in one batch interval, \n",
    "    # save to database directly without hotspots to avoid data duplication.        \n",
    "    if geohash in geohash_dictionary:\n",
    "        climate_temp.append(jsonData)\n",
    "    else:\n",
    "        geohash_dictionary.update({geohash:jsonData})\n",
    "    \n",
    "def hotspot_handler(geohash_dictionary, data):\n",
    "    hotspot = {}\n",
    "    hotspot['hotspot_station'] = str(data.get('station'))\n",
    "    hotspot['latitude'] = float(data.get('latitude'))\n",
    "    hotspot['longitude'] = float(data.get('longitude'))\n",
    "    hotspot['time'] = data.get('create_date')\n",
    "    hotspot['confidence'] = float(data.get('confidence'))\n",
    "    hotspot['surface_temperature_celcius'] = float(data.get('surface_temperature_celcius'))\n",
    "\n",
    "    # Map hotspot records based on geohash value\n",
    "    geohash = Geohash.encode(hotspot['latitude'], hotspot['longitude'], precision=5)\n",
    "\n",
    "    # For hotspots for the same location, average the data and save as one record.\n",
    "    if geohash in geohash_dictionary:\n",
    "\n",
    "        # data for the last average records in the same location for one interval\n",
    "        station = geohash_dictionary[geohash]['hotspot_station']\n",
    "        confidence = geohash_dictionary[geohash]['confidence']  # last data in the same location\n",
    "        surface_temperature = geohash_dictionary[geohash]['surface_temperature_celcius']\n",
    "\n",
    "        # average with the new record. There is a maximum of two fires.\n",
    "        geohash_dictionary[geohash]['hotspot_station'] = station + ', ' + hotspot['hotspot_station']\n",
    "        geohash_dictionary[geohash]['confidence'] = (confidence + hotspot['confidence'])/2\n",
    "        geohash_dictionary[geohash]['surface_temperature_celcius'] = (surface_temperature + \\\n",
    "                                                                     hotspot['surface_temperature_celcius'])/2\n",
    "    else:\n",
    "        dictionary.update({geohash:jsonData})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StoreData(data):\n",
    "    # start connection to database\n",
    "    client = MongoClient()\n",
    "    db = client.fit5148_assignment\n",
    "    collStream = db.stream\n",
    "    \n",
    "    # insert records to mongodb\n",
    "    try:\n",
    "        collStream.insert_many(data)\n",
    "    except Exception as ex:\n",
    "        print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# method to retrieve data and send to MongoDB.\n",
    "def Process(iter):\n",
    "    \n",
    "    # initial control variables\n",
    "    climate_temp = []\n",
    "    fire_temp = []\n",
    "    geohash_climate_dict = {}\n",
    "    geohash_fire_dict = {}\n",
    "    \n",
    "    for record in iter:\n",
    "        key = record[0]\n",
    "        data = json.loads(record[1])\n",
    "        \n",
    "         # retrieve climate records\n",
    "        if key == 'climate':\n",
    "            climate_handler(geohash_climate_dict,climate_temp,data)\n",
    "            \n",
    "        # retrieve hotspot data to json\n",
    "        elif key == 'hotspot':\n",
    "            hotspot_handler(geohash_fire_dict, data)\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"No handler for \" + key)\n",
    "            \n",
    "    # Embed hotspot records to climates according to geo location. Discard hotspots without a match\n",
    "    for geohash, hotspot in geohash_fire_dict.items():\n",
    "        if geohash in geohash_climate_dict:\n",
    "            geohash_climate_dict[geohash]['fire'].append(hotspot)\n",
    "    \n",
    "    # Merge cliamte records\n",
    "    climate_temp = climate_temp + list(geohash_climate_dict.values())\n",
    "\n",
    "    StoreData(climate_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data receiving...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0b4f29dc2873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data receiving...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Run stream for 10 minutes just in case no detection of producer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timeout\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# ssc.awaitTermination()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36msignal_handler\u001b[0;34m(signal, frame)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msignal_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelAllJobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;31m# see http://stackoverflow.com/questions/23206787/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# configuration for local streaming context with two execution threads and a batch interval of 10 seconds\n",
    "n_secs = 10\n",
    "\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, n_secs)\n",
    "\n",
    "topic = 'ClimateData' # choose the topic set by Kafka producers\n",
    "    \n",
    "# initialise the kafka stream\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092', \n",
    "                        'group.id':'Assignment', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "\n",
    "# manipulate each RDD from the kafka stream\n",
    "lines = kafkaStream.foreachRDD(lambda rdd: rdd.foreachPartition(Process))\n",
    "\n",
    "ssc.start()\n",
    "print(\"Data receiving...\")\n",
    "time.sleep(600) # Run stream for 10 minutes just in case no detection of producer\n",
    "print(\"Timeout\")\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to check for results.\n",
    "# Stop streaming first OR run in a new page.\n",
    "\n",
    "# from pymongo import MongoClient\n",
    "# from pprint import pprint\n",
    "\n",
    "# client = MongoClient()\n",
    "# db = client.fit5148_assignment\n",
    "# collStream = db.stream\n",
    "\n",
    "# count = collStream.find().count()\n",
    "# results = collStream.aggregate(\n",
    "#     [\n",
    "#         {\"$project\": {\"date\": 1, \"latitude\":1, \"longitude\":1, \"fire\":1,\"total_fire\":{\"$size\": \"$fire\"},\"_id\": 0 }},\n",
    "#         {\"$sort\": {'total_fire':-1}},\n",
    "#         {\"$limit\":10}\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# print(\"Total records: \" + str(count))\n",
    "# print(\"Top 10:\")\n",
    "# for r in results:\n",
    "#     pprint(r)\n",
    "\n",
    "# client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
