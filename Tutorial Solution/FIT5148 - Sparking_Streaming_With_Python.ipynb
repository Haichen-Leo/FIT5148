{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FIT5148 - Sparking_Streaming_With_Python.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"sRJa6F6sOEdl","colab_type":"text"},"cell_type":"markdown","source":["# FIT5148 - Big data management and processing\n","\n","# Activity: Spark Streaming with Python#\n","\n","**Apache Spark** is a fast and general engine for large-scale data processing. It has been reported that Spark is **100x faster** than Hadoop MapReduce in memory and **10x faster** on disk. Apache Spark is designed to write applications quickly in Java, Scala or Python.\n","\n","**Apache Spark Streaming** makes it easy to build scalable fault-tolerant **streaming** applications. Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams.\n","\n","In this activity, we will learn how to **write Spark Streaming programs in Python** using **discretized stream** or **DStreams** which represents a continuous stream of data. \n","\n","Let's get started!"]},{"metadata":{"id":"HFRlMtRDOEdm","colab_type":"text"},"cell_type":"markdown","source":["## 1. Overview ##\n","\n","### What is Apache Spark?\n","Apache Spark is a fast and general engine for big data processing and a distributed processing framework.\n","\n","It aims to provide a big data processing framework that can be used for streaming data manipulation (Spark streaming), machine learing and batch processing (Hadoop integration). Spark introduces an **abstract common data format** used to for efficient data sharing across parallel computation - **RDD (Resilient Distributed Datasets)**.\n","\n","### What is Apache Spark Streaming?\n","Spark Streaming provides a high-level abstraction called **discretized stream** or **DStream (a sequence of RDD)**, which represents a continuous stream of data. **Streaming data** can be brought from many difference live streams or sources (e.g. Twitter, Kafka). Then, the processed data can be manipulated and stored into a big database and/or published into Web pages.\n","\n","Processing streaming data is a new way of looking at and manipulating real-time streaming data which contradits batching processing. By processing streaming data, as one of the obvious benefits, we can reduce latency between an event occurring and taking an action driven by it.\n","\n","Once real-time input data streams are received, Spark Streaming divides the data into \"batches\", and then the Spark Engine process them. In this activity, we will learn and practice how we can manipulate input data streams in Python."]},{"metadata":{"id":"7wIni8ocOEdo","colab_type":"text"},"cell_type":"markdown","source":["## 2. Create Streaming Context ##\n","\n","### Our Example\n","To explain the use of the Spark APIs of Python, we will demonstrate a simple example:  ***\"counting the number of words in input data streams\"***.\n","\n","Imagine we are receiving the input text data streams through a TCP socket from a certain data server, and we wish to count the number of words in the data.\n","\n","### SparkContext and StreamingContext\n","Apache Spark community released a powerful Python package, **`pyspark`**. Using **`pyspark`**, we can  initialise Spark, load streaming data, create RDD  from the data, sort, filter and sample the data. \n","\n","Especially, we will use and import **`StreamingContext`** from **`pyspark`**, which is the main entry point for Spark Streaming functionality. The **`StreamingContext`** object provides methods used to create DStreams from various input sources.  \n","\n","Spark applications run as independent sets of processes on a cluster, which is specified by the **`SparkContext`** object. **`SparkContext`** can connect to several types of cluster managers (local (standalone), Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (passed to `SparkContext`) to the executors. Finally, **`SparkContext`** sends tasks to the executors to run.\n","\n","### Python Code\n","Thus, we need to import these two context:\n","\n","```\n","from pyspark import SparkContext # spark\n","from pyspark.streaming import StreamingContext # spark streaming\n","```\n","\n","As mentioned, **`SparkContext`** is the main object under which everything else can be used. Then, we need to pass this object with a batch interval (in this example, we use **10 seconds**) into the **`StreamingContext`** object. By doing so, we're ready to create our own stream context via `StreamingContext`:\n","\n","```\n","# Create a local StreamingContext with as many working processors as possible and a batch interval of 10 seconds            \n","batch_interval = 10\n","\n","# local[*]: run Spark locally with as many working processors as logical cores on your machine.\n","sc = SparkContext(master=\"local[*]\", appName = \"WordCountApp\") \n","\n","# a batch interval of 10 seconds   \n","ssc = StreamingContext(sc, batch_interval)\n","```\n","\n","In the field of `master`, we use a local server with as many working processors (or threads) as possible (i.e. `local[*]`). If we want Spark to run locally with 'k' worker threads, we can specify as `local[k]`.\n","\n","The `appName` field is a name to be shown on the Sparking cluster UI. The batch interval (i.e. `batch_interval`) must be set based on the latency requirements of your application and available cluster resources.\n"]},{"metadata":{"id":"gxaEY6_ROEdp","colab_type":"text"},"cell_type":"markdown","source":["## 3. Create DStream Data\n","\n","Once a `StreamingContext` (i.e. `ssc`) is defined, we can now define a DStreams representing the streaming data that can be received from a data server through a TCP socket. This server is specified in the method `ssc.socketTextStream(host, port)`, where `host` indicates the host name and `port` is its port number. With this example, the host is the local host and the port is 9999.\n","\n","```\n","# Create a DStream connecting to hostname:port\n","host = \"localhost\"\n","port = 9999\n","lines = ssc.socketTextStream(host, port)\n","```\n","\n","The variable `lines` represents the stream of data (i.e. DStream) that will be received from the data server. A unit record in this data corresponds to a line of text. \n","\n","To count the number of the words in each line, we may want to define a function that can split the line into words. With this example, we use a lambda function;\n","\n","```\n","# Split each line into words\n","words = lines.flatMap(lambda line: line.split(\" \"))\n","```\n","\n","`flatMap` is a one-to-many DStream operation. It creates a new DStream by generating multiple new records from each record. Thus, each line will be split into multiple words and we create a new DStream which is the stream of words. \n","\n","Now we further create a DStream of pairs (ie. the `pairs` DStream consisting of (word, count) pairs). For this purpose, we can use `reduceByKey` transformation for counting the number of each word in the `pairs` DStream. We can implement as follows:\n","```\n","# Count each word in each batch\n","pairs = words.map(lambda word: (word, 1))\n","wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n","# Print the counting result\n","wordCounts.pprint()\n","```"]},{"metadata":{"id":"t8j_BnxnOEdp","colab_type":"text"},"cell_type":"markdown","source":["## 4. Run Sparking Stream\n","\n","Note that up to now, we have only established a computation environment for our Spark Streaming example. Thus, no real processing has started yet. To start processing, we need to perform the following code:\n","\n","```\n","# Start the computation\n","ssc.start()             \n","# Wait for the computation to terminate. \n","# We have added a `timeout` to deliberately cancel the execution after one minute. \n","# In practice, you would not set this.\n","\n","try:\n","    ssc.awaitTermination(timeout=60)  \n","except KeyboardInterrupt:\n","    ssc.stop()\n","    \n","# If we want to manually stop the streaming context, use the following.\n","ssc.stop()\n","```"]},{"metadata":{"id":"NYYRgKLQpUDM","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"eh9d9sODOEdq","colab_type":"text"},"cell_type":"markdown","source":["### Important Note\n","We need to wrap up all the above code snippets as below. It is a **Streaming Client** program. This program counts the words in the line sent by the **Streaming Server** application. Before running the **Streaming Client**, we need to run a **Streaming Server** application. Please download **FIT5148 - TCP_Server.ipnyb** file from Moodle and open it in another tab. Run the **FIT5148 - TCP_Server.ipnyb** code. Then, run the code below. The lines sent from the TCP Server will be counted and printed on this browser every 10 seconds."]},{"metadata":{"id":"ZLgI2bKaOEdr","colab_type":"code","colab":{}},"cell_type":"code","source":["import sys\n","\n","from pyspark import SparkContext\n","from pyspark.streaming import StreamingContext\n","\n","# We add this line to avoid an error : \"Cannot run multiple SparkContexts at once\". \n","# If there is an existing spark context, we will reuse it instead of creating a new context.\n","sc = SparkContext.getOrCreate()\n","\n","# Create a local StreamingContext with as many working processors as possible \n","# and a batch interval of 10 seconds            \n","batch_interval = 10\n","\n","# If there is no existing spark context, we now create a new context\n","if (sc is None):\n","    sc = SparkContext(master=\"local[*]\", appName = \"WordCountApp\")\n","ssc = StreamingContext(sc, batch_interval)\n","\n","host = \"localhost\"\n","port = 9999\n","\n","lines = ssc.socketTextStream(host, int(port))\n","\n","# Split each line into words\n","words = lines.flatMap(lambda line: line.split(\" \"))\n","\n","# Count each word in each batch\n","pairs = words.map(lambda word: (word, 1))\n","wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n","\n","# Print the result                            \n","wordCounts.pprint()\n","\n","ssc.start()\n","try:\n","    ssc.awaitTermination(timeout=60)\n","except KeyboardInterrupt:\n","    ssc.stop()\n","    sc.stop()\n","\n","ssc.stop()\n","sc.stop()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1MMg9fu_OEdw","colab_type":"text"},"cell_type":"markdown","source":["## 5. Concepts in Sparking Streaming##\n","\n","Now we will learn some basic concepts in Spark Streaming. \n","\n","### Discretized Streams (DStreams)\n","As mentioned above, **DStream** is the basic abstraction in Spark Streaming. It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. \n","\n","A DStream is seen as a continuous series of **RDDs**, which is Spark's abstraction of an immutable, distributed dataset (see [Spark Programming](https://spark.apache.org/docs/latest/rdd-programming-guide.html) to learn its more details). Each RDD in a DStream contains data from a certain interval.\n","\n","Any operation applied on a DStream translates to operations on the underlying RDDs. For example, in our above example, the `flatMap` operation is applied on each RDD to generate the RDDs of the `words` DStream. \n","\n","\n","### Transformations on DStreams\n","We can apply various transformation operations on a DStream to modify its structure. Below, we see some of these transformations.\n","\n","#### UpdateStateByKey Operation\n","This operation allows us to maintain **arbitrary state** while continuously updating it with new information. \n","\n","In order to use this operation, we need to do the following: \n","    1. Define the state\n","    2. Define the state update function: specify with a function how to update the state \n","\n","To illustrate, let's get back to our previous example. Now we want to keep a count of each word seen in a text data stream. Here, **the running count is the state** and we will use **the `updateStateByKey` operation** for this update purpose:"]},{"metadata":{"id":"uIPIfk5TOEdx","colab_type":"code","colab":{}},"cell_type":"code","source":["from pyspark import SparkContext\n","from pyspark.streaming import StreamingContext\n","\n","# add the new values with the previous running count to get the new count\n","def updateFunc(new_values, prev_running_count):\n","    return sum(new_values) + (prev_running_count or 0)\n","  \n","# Create a local StreamingContext with as many working processors as possible and a batch interval of 10 seconds            \n","batch_interval = 10\n","\n","# We add this line to avoid an error : \"Cannot run multiple SparkContexts at once\". If there is an existing spark context, we will reuse it instead of creating a new context.\n","sc = SparkContext.getOrCreate()\n","\n","# If there is no existing spark context, we now create a new context\n","if (sc is None):\n","    sc = SparkContext(master=\"local[*]\", appName = \"WordCountApp\")\n","ssc = StreamingContext(sc, batch_interval)\n","ssc.checkpoint(\"checkpoint\")\n","\n","host = \"localhost\"\n","port = 9999\n","\n","lines = ssc.socketTextStream(host, int(port))\n","\n","# Split each line into words\n","words = lines.flatMap(lambda line: line.split(\" \"))\n","\n","# Count each word in each batch\n","pairs = words.map(lambda word: (word, 1))\n","wordCounts = pairs.updateStateByKey(updateFunc)\n","\n","# Print the result                            \n","wordCounts.pprint()\n","\n","ssc.start()\n","try:\n","    ssc.awaitTermination(timeout=60)\n","except KeyboardInterrupt:\n","    ssc.stop()\n","    sc.stop()\n","\n","ssc.stop()\n","sc.stop()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KoO3ir6BOEd0","colab_type":"text"},"cell_type":"markdown","source":["Can you see the effect of using the `updateStateByKey` operation? YES it's function is obvious. This operation is calling a function (`updateFunc`).  The `updateFunc` function has two parameters: \n","    1. `new_values` having a sequence of (word, 1) pairs \n","    2. `prev_running_count` having the previous count information of the pairs. \n","\n","\n","Note that the `updateStateByKey` opertion needs the checkpoint directory to be configured. \n","\n","##### Checkpointing\n","\n","A streaming application must run 24 hours a day. Thus, it needs to be resilient to failures caused by some unexpected errors such as system failures, driver failure, JVM crashes, etc. Checkpointing saves the generated RDDs to a reliable storate and performs receovery from an error. \n","\n","To summarise, checkpoints provide a way of recovering to a safe stable application snapshot. Using the `ssc.checkpoint()` method, we can tell the Spark engine **where to store the checkpoint files**."]},{"metadata":{"id":"iMY3ltBBOEd0","colab_type":"text"},"cell_type":"markdown","source":["### Window Operation\n","Spark Streaming also provides windowed computations. This function allows to apply transformations over a sliding window of data. \n","\n","Every time the window slides over a source DStream. Thus, the source RDDs that fall within the window are combined and operated to produce the RDDs of the windowed DStream. \n","\n","A window operation needs two parameters:\n","    1. window length: the duration of the window.\n","    2. sliding interval: the interval at which the window operation is performed.\n","\n","These two parameters must be multiples of the batch interval (i.e. in our example: 10 sec) the source DStream.\n","\n","To illustrate, refer to our previous example. If we want to generate word counts over the last 20 seconds of data, every 10 seconds, we can use the following command:\n","\n","```\n","windowedWordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 20, 10)\n","```\n","\n","<font color='blue'>\n","**Exercise**: Apply the reduceByKeyAndWindow operation, and check how it is working!\n","</font><br>"]},{"metadata":{"id":"OpZIehxXOEd2","colab_type":"text"},"cell_type":"markdown","source":["### Join Operations\n","\n","Also, we can easily join two different streams into one stream data in Spark Streaming.\n","\n","For example, if we want to join the `stream2` data into the `stream1` data, we can use the following code: \n","\n","```\n","stream1 = ...\n","stream2 = ...\n","joinedStream = stream1.join(stream2)\n","```"]},{"metadata":{"id":"i_b38urMOEd3","colab_type":"text"},"cell_type":"markdown","source":["### Output Operations on DStreams ##\n","\n","When we want to send DStream to an external system or database, we can use various output operations. The following output operations can be used:\n","\n","    - print(): print the first ten elements of every batch of data in a DStream running the streaming application. In Python, pprint() corresponds to print().\n","    - saveAsTextFiles(prefix, [suffix]): save the DStream data as text files. The file name at each batch interval is generated based on prefix and suffix: \"prefix-TIME_IN_MS[.suffix]\".\n","    - foreachRDD(func): Each RDD in DStream can be pushed out using this method. Note that the function `func` is executed on the running the streaming application, and will usually have RDD actions."]},{"metadata":{"id":"TNrzDEqcOEd4","colab_type":"text"},"cell_type":"markdown","source":["For example, with our original example, on the `wordCounts` DStream, we can use the following code:\n","\n","```\n","def sendPartition(iter):\n","    connection = createNewConnection() # Assuming such fucntion exists\n","    for record in iter:\n","        connection.send(record)\n","    connection.close()\n","    \n","wordCounts.foreachRDD(lambda rdd: rdd.foreachPartition(sendPartition))\n","```\n","\n","In `sendPartition()`, we create a single connection object and send all the records in a RDD partition using that connection.\n","\n","As an example, if we can store each RDD into a MongoDB database, for example the `test_db`, then we can use the following code in the `sendPartition()` function:\n","\n","```\n","connection = MongoClient()\n","test_db = connection.get_database('test_db')\n","....\n","```\n","You will learn more on this topic in next tutorial."]},{"metadata":{"id":"6S_xyisqOEd5","colab_type":"text"},"cell_type":"markdown","source":["## Summary\n","\n","Congratulations on finishing this activity!\n","\n","<font color='blue'>\n","**Wrap up what we've learned:**\n","- Learned that Spark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data.\n","- Learned that Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results\n","- Learned that using \"StreamingContext\", we can define the input sources by creating input DStreams; apply transformation and output operations to DStreams; and receive data and process it.\n","- Learned that the \"updateStateByKey\" operation allows you to maintain arbitrary state while continuously updating it with new information. \n","- Learned how to use \"dstream.foreachRDD\" that allows data to be sent out to external systems."]}]}