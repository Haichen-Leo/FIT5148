{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oD5VW0TzHdR_"
   },
   "source": [
    "# FIT5148 - Distributed Databases and Big Data\n",
    "\n",
    "# Take Home Test - Solution Workbook#\n",
    "\n",
    "This test consists of three questions total worth 5% of the final marks. The first question is related to ** Parallel Search Algorithms (1 Marks)**, the second question is related to ** Parallel Join Algorithms (2 Marks)** and the third question is realted to ** Parallel Sort and GroupBy Algorithms (2 Marks)**.\n",
    "\n",
    "**Instructions:**\n",
    "- You will be using Python 3.\n",
    "- Read the instructions, code base and comments carefully.\n",
    "- There are code blocks that **you need to complete** yourself as a part of test.\n",
    "- <font color='red'> **Comment each line of code properly such that the tutor can easily understand what you are trying to do in the code.**</font>\n",
    "\n",
    "**Your Details:**\n",
    "- Name: Haichen Wang\n",
    "- StudentID: 29212634\n",
    "- Email: hwan0066@student.monash.edu\n",
    "\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRWxd1YrHdSB"
   },
   "source": [
    "\n",
    "\n",
    "### Dataset ###\n",
    "For this test, we will use the following two tables R and S to write the solutions to three parallel algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImhBOqgmHdSC"
   },
   "outputs": [],
   "source": [
    "# R consists of 15 pairs, each comprising two attributes (nominal and numeric)\n",
    "R = [('Adele',8),('Bob',22),('Clement',16),('Dave',23),('Ed',11),\n",
    "     ('Fung',25),('Goel',3),('Harry',17),('Irene',14),('Joanna',2),\n",
    "     ('Kelly',6),('Lim',20),('Meng',1),('Noor',5),('Omar',19)]\n",
    "\n",
    "# S consists of 8 pairs, each comprising two attributes (nominal and numeric)\n",
    "S = [('Arts',8),('Business',15),('CompSc',2),('Dance',12),('Engineering',7),\n",
    "     ('Finance',21),('Geology',10),('Health',11),('IT',18)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7W3uUvJLErBY"
   },
   "source": [
    "### 1. Parallel Searching Algorithm ###\n",
    "In this task, you will build a **parallel search algorithm for range selection (continuous)** for a given query. You will implement one particular search algorithm which is instructed below.\n",
    "\n",
    " **Implement a parallel search algorithm** that uses the linear search algorithm (i.e. **`linear_search()`**) and is able to work with the hash partitioning method (i.e.**` h_partition()`**). \n",
    " **Complete the code block between \"### START CODE HERE ###\" and \"### END CODE HERE ###\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CvTYiOkyFJp8"
   },
   "outputs": [],
   "source": [
    "# Linear search function\n",
    "def linear_search(data, key):\n",
    "    \"\"\"\n",
    "    Perform linear search on data for the given key\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list or a numpy array\n",
    "    key -- an query record\n",
    "\n",
    "    Return:\n",
    "    result -- the position of searched record\n",
    "    \"\"\"\n",
    "    \n",
    "    matched_records = []\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    # initialise position\n",
    "    position = -1\n",
    "    matched_record = None\n",
    "    \n",
    "    for x in data:\n",
    "        if x[1] == key:\n",
    "            position = data.index(x)\n",
    "            matched_record = x\n",
    "            # append matched record and corresponding position\n",
    "            matched_records.append((position,matched_record)) \n",
    "  \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return matched_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "An_0xFW2FQvs"
   },
   "outputs": [],
   "source": [
    "# Define a simple hash function.\n",
    "def s_hash(x, n):\n",
    "    \"\"\"\n",
    "    Define a simple hash function for demonstration\n",
    "\n",
    "    Arguments:\n",
    "    x -- an input record\n",
    "    n -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the hash value of x\n",
    "    \"\"\"\n",
    "    result = x%n \n",
    " \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMLZCK2BFYeF"
   },
   "outputs": [],
   "source": [
    "# Hash data partitionining function. \n",
    "# We will use the \"s_hash\" function defined above to realise this partitioning\n",
    "def h_partition(data, n):\n",
    "    \"\"\"\n",
    "    Perform hash data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    n -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    partitions = {}\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    for element in data:\n",
    "        h_value = s_hash(element[1],n) # generate the hash value of numeric attribute of each element\n",
    "        if h_value not in partitions:\n",
    "            s = set()  # create a new value set\n",
    "            \n",
    "        else:\n",
    "            s = partitions[h_value]\n",
    "        \n",
    "        s.add(element)  # add new value to the set\n",
    "        partitions[h_value] = s  # update the key-value pair\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlVKTCO-FkV9"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "# Parallel searching algorithm for range selection\n",
    "def parallel_search_range(data, query_range, n_processor):\n",
    "    \"\"\"\n",
    "    Perform parallel search for range selection on data for the given key\n",
    "\n",
    "    Arguments:\n",
    "    data -- the input dataset which is a list\n",
    "    query_range -- a query record in the form of a range (e.g. [30, 50])\n",
    "    n_processor -- the number of parallel processors\n",
    "    \n",
    "    Return:\n",
    "    results -- the matched record information\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    pool = Pool(processes=n_processor)\n",
    "\n",
    "    ### START CODE HERE ###        \n",
    "    \n",
    "    # Perform data partitioning first\n",
    "    data_partitioned = h_partition(data,n_processor)\n",
    "    \n",
    "    for query in range(query_range[0],query_range[1],1):\n",
    "        # for each query in range, do\n",
    "        query_hash = s_hash(query,n_processor)  # create the hash value of a query\n",
    "        subdata = list(data_partitioned[query_hash])  # list partitioned data given a query\n",
    "        result = pool.apply_async(linear_search,[subdata,query])  # linear search for each subdata in process pool\n",
    "        output = result.get()\n",
    "        results.append(output)\n",
    "        \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vgfN1IcyFxaG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(5, ('Noor', 5))], [(1, ('Kelly', 6))], [], [(6, ('Adele', 8))], [], [], [(4, ('Ed', 11))], [], [], [(3, ('Irene', 14))], [], [(4, ('Clement', 16))], [(2, ('Harry', 17))], [], [(3, ('Omar', 19))]]\n"
     ]
    }
   ],
   "source": [
    "n_processor = 3\n",
    "# Range partition, linear_search \n",
    "results = parallel_search_range(R, [5, 20], n_processor)\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qf29MfmUGWV1"
   },
   "source": [
    "## 2. Parallel Join Algorithm\n",
    "\n",
    "In this task, you will implement a **disjoint-partitioning based parallel join algorithm**. This algorithm consist of two stages: a data partitioning stage using a disjoint partitioning and a local join.\n",
    "\n",
    " \n",
    "As a data partitioning method, use the range partitioninig method  (i.e. **`range_partition( )`**).\n",
    "Assume that we have **3 parallel processors**, processor 1 will get records with join attribute value between 1 and 9, processor 2 between 10 and 19, and processor 3 between 20 and 29. Note that both tables R and S need to be partitioned based on the join attribute with the same range partitioning function. \n",
    "\n",
    "As a joining technique, use the hash based join algorithm (i.e.**`HB_join( )`** ).  **Complete the code block between \"### START CODE HERE ###\" and \"### END CODE HERE ###\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_BwmzrmHaTN"
   },
   "outputs": [],
   "source": [
    "# Range data partitionining function (Need to modify as instructed above)\n",
    "def range_partition(data, range_indices):\n",
    "    \"\"\"\n",
    "    Perform range data partitioning on data based on the join attribute\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    range_indices -- the index list of ranges to be s:plit\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    ### START CODE HERE ###  \n",
    "    # sort input data\n",
    "    sorted_data = list(data)\n",
    "    sorted_data.sort(key = lambda x: x[1]) # sort data with the numeric value\n",
    "    \n",
    "    # calculate number of ranges\n",
    "    no_ranges = len(range_indices)\n",
    "    \n",
    "    for index in range(no_ranges):\n",
    "        s = []\n",
    "        # add all elements in range to s\n",
    "        for x in sorted_data:\n",
    "            if x[1] >= 1 and x[1] < range_indices[index]:\n",
    "                s.append(x)\n",
    "        # add partitioned result to result\n",
    "        result.append(s)\n",
    "        # find the last element partitioned\n",
    "        last_element = s[len(s)-1]\n",
    "        last_index = sorted_data.index(last_element)\n",
    "        sorted_data = sorted_data[int(last_index)+1:]  # update sorted_data\n",
    "    # find the last elements list\n",
    "    for x in sorted_data:\n",
    "        if x[1] < range_indices[no_ranges-1] or x[1] > 29:\n",
    "            sorted_data.remove(x)\n",
    "    result.append(sorted_data)\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kONrahsIMmD"
   },
   "outputs": [],
   "source": [
    "def H(r):\n",
    "    \"\"\"\n",
    "    We define a hash function 'H' that is used in the hashing process works \n",
    "    by summing the first and second digits of the hashed attribute, which\n",
    "    in this case is the join attribute. \n",
    "    \n",
    "    Arguments:\n",
    "    r -- a record where hashing will be applied on its join attribute\n",
    "\n",
    "    Return:\n",
    "    result -- the hash index of the record r\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the value of the join attribute into the digits\n",
    "    digits = [int(d) for d in str(r[1])]\n",
    "    \n",
    "    # Calulate the sum of elemenets in the digits\n",
    "    return sum(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gEpFbToJIPlr"
   },
   "outputs": [],
   "source": [
    "def HB_join(T1, T2):\n",
    "    \"\"\"\n",
    "    Perform the hash-based join algorithm.\n",
    "    The join attribute is the numeric attribute in the input tables T1 & T2\n",
    "\n",
    "    Arguments:\n",
    "    T1 & T2 -- Tables to be joined\n",
    "\n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    \n",
    "    dic = {} # We will use a dictionary\n",
    "    \n",
    "    # For each record in table T2\n",
    "    for s in T2:\n",
    "        # Hash the record based on join attribute value using hash function H into hash table\n",
    "        s_key = H(s)\n",
    "        if s_key in dic:\n",
    "            dic[s_key].add(s) # If there is an entry\n",
    "        else:\n",
    "            dic[s_key] = {s}\n",
    "            \n",
    "    # For each record in table T1 (probing)\n",
    "    for r in T1:\n",
    "        # Hash the record based on join attribute value using H\n",
    "        r_key = H(r)\n",
    "\n",
    "        # If an index entry is found Then\n",
    "        if r_key in dic:\n",
    "            # Compare each record on this index entry with the record of table T1\n",
    "            for item in dic[r_key]:\n",
    "                if item[1] == r[1]:\n",
    "                    # Put the rsult\n",
    "                    result.append({\", \".join([r[0], str(r[1]), item[0]])})\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9cpQYKvvH241"
   },
   "outputs": [],
   "source": [
    "# Include this package for parallel processing\n",
    "import multiprocessing as mp\n",
    "\n",
    "def DPBP_join(T1, T2, n_processor):\n",
    "    \"\"\"\n",
    "    Perform a disjoint partitioning-based parallel join algorithm.\n",
    "    The join attribute is the numeric attribute in the input tables T1 & T2\n",
    "\n",
    "    Arguments:\n",
    "    T1 & T2 -- Tables to be joined\n",
    "    n_processor -- the number of parallel processors\n",
    "\n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Partition T1 & T2 into sub-tables using range_partition().\n",
    "    # The number of the sub-tables must be the equal to the n_processor\n",
    "    T1_subsets = range_partition(T1, [10, 20])\n",
    "    T2_subsets = range_partition(T2, [10, 20])\n",
    "    # Initialise process pool for parallel processing\n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "    # create a temp list\n",
    "    temp = []\n",
    "    # HB_join sub-tables based on range_indices in parallel.\n",
    "    for i in range(len(T1_subsets)):\n",
    "        output = pool.apply_async(HB_join,[T1_subsets[i],T2_subsets[i]])\n",
    "        temp.append(output)\n",
    "    # Append result in temp list to results.    \n",
    "    for result in temp:\n",
    "        results.append(result.get())\n",
    "    \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJgTe8pVH_0z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'Joanna, 2, CompSc'}, {'Adele, 8, Arts'}], [{'Ed, 11, Health'}], []]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_processor = 3\n",
    "DPBP_join(R, S, n_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9vIqdbjJaXv"
   },
   "source": [
    "## 3. Parallel Sorting Algorithm\n",
    "\n",
    "In this task, you will implement **parallel binary-merge sort** method. It has two phases same as the parallel merge-all sort that you learnt in the labs: (1) local sort and (2) final merge. The first phase is similar to the parallel merge-all sort. The second phase, the merging phase, is pipelined instead of concentrating on one processor. In this phase, we take the results from two processors and then merging the two in one processor, called binary merging. The result of the merging between two processors is passed on to the next level until one processor (the host) is left.\n",
    "\n",
    " **Complete the code block between \"### START CODE HERE ###\" and \"### END CODE HERE ###\".**\n",
    "Assume that we use the round robin partitioning method  (i.e. **`rr_partition()`**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gd16AZF_LgWp"
   },
   "outputs": [],
   "source": [
    "# You will have to edit qsort(arr) to make it work.\n",
    "def qsort(arr): \n",
    "\n",
    "    \"\"\" \n",
    "    Quicksort a list\n",
    "    \n",
    "    Arguments:\n",
    "    arr -- the input list to be sorted\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted arr\n",
    "    \"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    else:\n",
    "        #take the first element as the pivot\n",
    "        pivot = arr[0]\n",
    "        left_arr = [x for x in arr[1:] if x[1] < pivot[1]] # add elements with smaller numeric value to left_arr\n",
    "        right_arr = [x for x in arr[1:] if x[1] >= pivot[1]] # add elements with bigger numeric value to right_arr\n",
    "        # uncomment this to see what to print \n",
    "        # print(\"Left:\" + str(left_arr)+\" Pivot : \"+ str(pivot)+\" Right: \" + str(right_arr))\n",
    "        value = qsort(left_arr) + [pivot] + qsort(right_arr)\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m3vxcrs-LVaG"
   },
   "outputs": [],
   "source": [
    "# You will have to edit find_min(records) and k_way_merge(record_sets) to make it work.\n",
    "import sys\n",
    "\n",
    "# Find the smallest record\n",
    "def find_min(records):    \n",
    "    \"\"\" \n",
    "    Find the smallest record\n",
    "    \n",
    "    Arguments:\n",
    "    records -- the input record set\n",
    "\n",
    "    Return:\n",
    "    result -- the smallest record's index\n",
    "    \"\"\"\n",
    "    m = records[0]\n",
    "    index = 0\n",
    "    for i in range(len(records)):\n",
    "        if(records[i][1] < m[1]):  # Edit is required here\n",
    "            index = i\n",
    "            m = records[i]\n",
    "    return index\n",
    "\n",
    "def k_way_merge(record_sets):\n",
    "    \"\"\" \n",
    "    K-way merging algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    record_sets -- the set of mulitple sorted sub-record sets\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted and merged record set\n",
    "    \"\"\"\n",
    "    \n",
    "    # indexes will keep the indexes of sorted records in the given buffers\n",
    "    indexes = []\n",
    "    for x in record_sets:\n",
    "        indexes.append(0) # initialisation with 0\n",
    "\n",
    "    # final result will be stored in this variable\n",
    "    result = []  \n",
    "    \n",
    "    while(True):\n",
    "        merged_result = [] # the merging unit (i.e. # of the given buffers)\n",
    "        \n",
    "        # This loop gets the current position of every buffer\n",
    "        for i in range(len(record_sets)):\n",
    "            if(indexes[i] >= len(record_sets[i])):\n",
    "                merged_result.append(('bound',sys.maxsize)) # run out of one record set\n",
    "            else:\n",
    "                merged_result.append(record_sets[i][indexes[i]])  \n",
    "        \n",
    "        # find the smallest record \n",
    "        smallest = find_min(merged_result)\n",
    "    \n",
    "        # if we only have sys.maxsize on the tuple, we reached the end of every record set\n",
    "        if(merged_result[smallest][1] == sys.maxsize): # if numeric value of all results = sys.maxsize\n",
    "            break\n",
    "\n",
    "        # This record is the next on the merged list\n",
    "        result.append(record_sets[smallest][indexes[smallest]])\n",
    "        indexes[smallest] +=1\n",
    "   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yof8Q84YLcOU"
   },
   "outputs": [],
   "source": [
    "def serial_sorting(dataset, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a serial external sorting method based on sort-merge\n",
    "    The buffer size determines the size of each sub-record set\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- the entire record set to be sorted\n",
    "    buffer_size -- the buffer size determining the size of each sub-record set\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted record set\n",
    "    \"\"\"\n",
    "    \n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # --- Sort Phase ---\n",
    "    sorted_set = []\n",
    "    \n",
    "    # Read buffer_size pages at a time into memory and\n",
    "    # sort them, and write out a sub-record set (i.e. variable: subset)\n",
    "    start_pos = 0\n",
    "    N = len(dataset)\n",
    "    while True:\n",
    "        if ((N - start_pos) > buffer_size):\n",
    "            # read B-records from the input, where B = buffer_size\n",
    "            subset = dataset[start_pos:start_pos + buffer_size] \n",
    "            # sort the subset (using qucksort defined above)\n",
    "            sorted_subset = qsort(subset) \n",
    "            sorted_set.append(sorted_subset)\n",
    "            start_pos += buffer_size\n",
    "        else:\n",
    "            # read the last B-records from the input, where B is less than buffer_size\n",
    "            subset = dataset[start_pos:] \n",
    "            # sort the subset (using qucksort defined above)\n",
    "            sorted_subset = qsort(subset) \n",
    "            sorted_set.append(sorted_subset)\n",
    "            break\n",
    "    \n",
    "    # --- Merge Phase ---\n",
    "    merge_buffer_size = buffer_size - 1\n",
    "    dataset = sorted_set\n",
    "    while True:\n",
    "        merged_set = []\n",
    "\n",
    "        N = len(dataset)\n",
    "        start_pos = 0\n",
    "        while True:\n",
    "            if ((N - start_pos) > merge_buffer_size): \n",
    "                # read C-record sets from the merged record sets, where C = merge_buffer_size\n",
    "                subset = dataset[start_pos:start_pos + merge_buffer_size]\n",
    "                merged_set.append(k_way_merge(subset)) # merge lists in subset\n",
    "                start_pos += merge_buffer_size\n",
    "            else:\n",
    "                # read C-record sets from the merged sets, where C is less than merge_buffer_size\n",
    "                subset = dataset[start_pos:]\n",
    "                merged_set.append(k_way_merge(subset)) # merge lists in subset\n",
    "                break\n",
    "\n",
    "        dataset = merged_set\n",
    "        if (len(dataset) <= 1): # if the size of merged record set is 1, then stop \n",
    "            result = merged_set\n",
    "            break\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_jH8jXwLKRT"
   },
   "outputs": [],
   "source": [
    "# Include this package for parallel processing\n",
    "import multiprocessing as mp\n",
    "\n",
    "def parallel_binary_merge_sorting(dataset, n_processor, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a parallel binary-merge sorting method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be sorted\n",
    "    n_processor -- number of parallel processors\n",
    "    buffer_size -- buffer size determining the size of each sub-record set\n",
    "\n",
    "    Return:\n",
    "    result -- the merged record set\n",
    "    \"\"\"\n",
    "    \n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Data partitioning by round-robin\n",
    "    subsets = []\n",
    "    for i in range(n_processor):\n",
    "        subsets.append([])\n",
    "    \n",
    "    for index, element in enumerate(dataset):  # add elements to bins equally\n",
    "        index_bin = (int)(index % n_processor)\n",
    "        subsets[index_bin].append(element)\n",
    "    \n",
    "    # initialise a pool for parallel processing\n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "    \n",
    "    # Sort Phase\n",
    "    sorted_set = []\n",
    "    for partitioned_data in subsets:\n",
    "        # serial_sorting at each processor\n",
    "        sorted_set.append(*pool.apply_async(serial_sorting, [partitioned_data, buffer_size]).get())\n",
    "    pool.close()\n",
    "    \n",
    "    # Final merge phase\n",
    "    print(\"sorted entire set:\" + str(sorted_set))\n",
    "    dataset = sorted_set\n",
    "    while True:\n",
    "        merged_set = []\n",
    "        \n",
    "        N = len(dataset)\n",
    "        start_pos = 0\n",
    "        pool = mp.Pool(processes = N//2) # N//2 - calculate # of processors for binary merge\n",
    "        \n",
    "        while True:\n",
    "            if ((N - start_pos) > 2): \n",
    "                subset = dataset[start_pos:start_pos + 2]\n",
    "                merged_set.append(pool.apply(k_way_merge, [subset]))\n",
    "                start_pos += 2\n",
    "            else:\n",
    "                subset = dataset[start_pos:]\n",
    "                merged_set.append(pool.apply(k_way_merge, [subset]))\n",
    "                break\n",
    "        \n",
    "        pool.close()\n",
    "        dataset = merged_set\n",
    "        \n",
    "        if (len(dataset) == 1): # stop when all subdata are merged into one\n",
    "            result = merged_set\n",
    "            break\n",
    "  \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aMu19MwXLxNd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted entire set:[[('Kelly', 6), ('Adele', 8)], [('Lim', 20), ('Bob', 22)], [('Meng', 1), ('Clement', 16)], [('Noor', 5), ('Dave', 23)], [('Ed', 11), ('Omar', 19)], [('Fung', 25)], [('Goel', 3)], [('Harry', 17)], [('Irene', 14)], [('Joanna', 2)]]\n",
      "Final Result:[[('Meng', 1), ('Joanna', 2), ('Goel', 3), ('Noor', 5), ('Kelly', 6), ('Adele', 8), ('Ed', 11), ('Irene', 14), ('Clement', 16), ('Harry', 17), ('Omar', 19), ('Lim', 20), ('Bob', 22), ('Dave', 23), ('Fung', 25)]]\n"
     ]
    }
   ],
   "source": [
    "result = parallel_binary_merge_sorting(R, 10, 20)\n",
    "print(\"Final Result:\" + str(result))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT5148 -Take_Home_Test.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
